{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "afe8acfdcab94780ba011fb52bf7dabc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff72e2f50c6a455ab63ebedef20889be",
              "IPY_MODEL_c0a8df2ed53f4fd088341eabb48e6461"
            ],
            "layout": "IPY_MODEL_a51551fbebbb48d29a536ad513808a93"
          }
        },
        "ff72e2f50c6a455ab63ebedef20889be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce333d0a4f054452ba351ba8bee86e38",
            "placeholder": "​",
            "style": "IPY_MODEL_986dc0cc371344b09ea6a97cb79acd23",
            "value": "2.259 MB of 2.259 MB uploaded\r"
          }
        },
        "c0a8df2ed53f4fd088341eabb48e6461": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43463f8ed70e4707a2adbaab7a4f3e6e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bef92a1503044080b366db7472ade38b",
            "value": 1
          }
        },
        "a51551fbebbb48d29a536ad513808a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce333d0a4f054452ba351ba8bee86e38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "986dc0cc371344b09ea6a97cb79acd23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43463f8ed70e4707a2adbaab7a4f3e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bef92a1503044080b366db7472ade38b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcsq6fWYB1LX",
        "outputId": "64958c98-0aa5-427c-bdd6-32e031e373eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.12.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Downloading wandb-0.17.6-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-2.12.0-py2.py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.12.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.6\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torchvision.transforms import functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np\n",
        "from torchvision.ops import box_iou\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wandb\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize WandB for experiment tracking\n",
        "api_key = \"9ce954fd827fd8d839648cb3708ff788ad51bafa\"\n",
        "wandb.login(key=api_key)\n",
        "wandb.init(project=\"yolov8-project-pretrained\", entity=\"enxo7899\")\n",
        "\n",
        "# Hyperparameters and configuration\n",
        "hyperparams = {\n",
        "    \"batch_size\": 8,            # Increased batch size for faster training if memory allows\n",
        "    \"num_workers\": 4,           # Increased workers for faster data loading\n",
        "    \"learning_rate\": 0.005,     # Learning rate for optimizer\n",
        "    \"momentum\": 0.9,            # Momentum for SGD\n",
        "    \"num_epochs\": 3,            # Reduced epochs for faster runs\n",
        "    \"iou_threshold\": 0.5,       # IoU threshold for evaluation\n",
        "}\n",
        "\n",
        "# Check if GPU is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a mapping from class names to integers\n",
        "CLASS_NAMES = [\n",
        "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
        "    \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\",\n",
        "    \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
        "]\n",
        "CLASS_NAME_TO_IDX = {name: i for i, name in enumerate(CLASS_NAMES)}\n",
        "\n",
        "# Download and prepare the VOC2012 dataset\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, year, image_set, transforms):\n",
        "        self.dataset = VOCDetection(root, year=year, image_set=image_set, download=True)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.dataset[idx][0]\n",
        "        target = self.dataset[idx][1]\n",
        "        target = self._convert_target(target)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def _convert_target(self, target):\n",
        "        # Convert the target from the VOC format to a format compatible with YOLOv8\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        objects = target['annotation']['object']\n",
        "        if isinstance(objects, dict):\n",
        "            objects = [objects]\n",
        "\n",
        "        for obj in objects:\n",
        "            bbox = obj['bndbox']\n",
        "            box = [\n",
        "                int(bbox['xmin']),\n",
        "                int(bbox['ymin']),\n",
        "                int(bbox['xmax']),\n",
        "                int(bbox['ymax'])\n",
        "            ]\n",
        "            boxes.append(box)\n",
        "            labels.append(CLASS_NAME_TO_IDX[obj['name']])\n",
        "\n",
        "        return {\n",
        "            'boxes': torch.as_tensor(boxes, dtype=torch.float32),\n",
        "            'labels': torch.as_tensor(labels, dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "# Define transformations\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(F.to_tensor)\n",
        "    return torchvision.transforms.Compose(transforms)\n",
        "\n",
        "# Initialize dataset and dataloaders\n",
        "full_dataset = VOCDataset(root='./data', year='2012', image_set='train', transforms=get_transform(train=True))\n",
        "test_dataset = VOCDataset(root='./data', year='2012', image_set='val', transforms=get_transform(train=False))\n",
        "\n",
        "# Split the dataset for training (70%) and validation (30%)\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Data loaders\n",
        "data_loader = DataLoader(train_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=True, num_workers=hyperparams[\"num_workers\"], collate_fn=lambda x: tuple(zip(*x)))\n",
        "data_loader_val = DataLoader(val_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=False, num_workers=hyperparams[\"num_workers\"], collate_fn=lambda x: tuple(zip(*x)))\n",
        "data_loader_test = DataLoader(test_dataset, batch_size=hyperparams[\"batch_size\"], shuffle=False, num_workers=hyperparams[\"num_workers\"], collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Load a pretrained YOLOv8 model\n",
        "model = YOLO('yolov8n.pt')\n",
        "model.to(device)\n",
        "\n",
        "# Function to save a model checkpoint\n",
        "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
        "    torch.save(state, filename)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = hyperparams[\"num_epochs\"]\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, targets) in enumerate(data_loader):\n",
        "        images = [image.to(device) for image in images]\n",
        "        results = model(images)  # Forward pass with YOLOv8\n",
        "\n",
        "\n",
        "\n",
        "        # Save checkpoint every epoch\n",
        "        save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict()}, f\"checkpoint_epoch_{epoch+1}.pth.tar\")\n",
        "\n",
        "# Function to calculate evaluation metrics\n",
        "def calculate_metrics(model, data_loader, device, iou_threshold=hyperparams[\"iou_threshold\"]):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [image.to(device) for image in images]\n",
        "            results = model(images)  # Forward pass with YOLOv8\n",
        "\n",
        "            for target, output in zip(targets, results):\n",
        "                true_boxes = target['boxes'].to(device)\n",
        "                true_labels = target['labels'].cpu().numpy()\n",
        "\n",
        "                pred_boxes = output.boxes.xyxy.to(device)\n",
        "                pred_labels = output.boxes.cls.cpu().numpy()\n",
        "                pred_scores = output.boxes.conf.cpu().numpy()\n",
        "\n",
        "                # Filter predictions by a score threshold (e.g., 0.5)\n",
        "                keep = pred_scores > 0.5\n",
        "                pred_boxes = pred_boxes[keep]\n",
        "                pred_labels = pred_labels[keep]\n",
        "\n",
        "                # Calculate IoU between predicted and true boxes\n",
        "                if len(pred_boxes) > 0 and len(true_boxes) > 0:\n",
        "                    ious = box_iou(pred_boxes, true_boxes)\n",
        "                    ious_max, indices = ious.max(dim=1)\n",
        "                    matched = ious_max > iou_threshold\n",
        "\n",
        "                    # Filter matched indices for valid IoU\n",
        "                    matched_true_indices = indices[matched].cpu().numpy()\n",
        "                    matched_pred_indices = np.where(matched.cpu().numpy())[0]\n",
        "\n",
        "                    # Collect the matched true and predicted labels\n",
        "                    y_true.extend(true_labels[matched_true_indices].tolist())\n",
        "                    y_pred.extend(pred_labels[matched_pred_indices].tolist())\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Log metrics to WandB\n",
        "    wandb.log({\"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plot_confusion_matrix(conf_matrix, CLASS_NAMES)\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names):\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.set(font_scale=1.2)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Calculate and print metrics on the validation dataset\n",
        "calculate_metrics(model, data_loader_val, device)\n",
        "\n",
        "# Finish WandB logging\n",
        "wandb.finish()\n",
        "\n",
        "# Inference function for a single image\n",
        "def inference(model, image_path, device):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path)\n",
        "    transform = get_transform(train=False)\n",
        "    image = transform(image).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        results = model(image)\n",
        "\n",
        "    # Extract predictions\n",
        "    pred_boxes = results.boxes.xyxy.cpu().numpy()\n",
        "    pred_labels = results.boxes.cls.cpu().numpy()\n",
        "    pred_scores = results.boxes.conf.cpu().numpy()\n",
        "\n",
        "    # Display results\n",
        "    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "        print(f\"Label: {CLASS_NAMES[int(label)]}, Score: {score}, Box: {box}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage of inference\n",
        "# inference_results = inference(model, './data/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg', device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "afe8acfdcab94780ba011fb52bf7dabc",
            "ff72e2f50c6a455ab63ebedef20889be",
            "c0a8df2ed53f4fd088341eabb48e6461",
            "a51551fbebbb48d29a536ad513808a93",
            "ce333d0a4f054452ba351ba8bee86e38",
            "986dc0cc371344b09ea6a97cb79acd23",
            "43463f8ed70e4707a2adbaab7a4f3e6e",
            "bef92a1503044080b366db7472ade38b"
          ]
        },
        "id": "fwbdonCvp47h",
        "outputId": "96a61771-bc69-436f-ee50-43a33b88727b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:6oj1rwer) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='2.238 MB of 2.238 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afe8acfdcab94780ba011fb52bf7dabc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>▁</td></tr><tr><td>lr/pg1</td><td>▁</td></tr><tr><td>lr/pg2</td><td>▁</td></tr><tr><td>metrics/mAP50(B)</td><td>▁</td></tr><tr><td>metrics/mAP50-95(B)</td><td>▁</td></tr><tr><td>metrics/precision(B)</td><td>▁</td></tr><tr><td>metrics/recall(B)</td><td>▁</td></tr><tr><td>model/GFLOPs</td><td>▁</td></tr><tr><td>model/parameters</td><td>▁</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>▁</td></tr><tr><td>train/box_loss</td><td>▁</td></tr><tr><td>train/cls_loss</td><td>▁</td></tr><tr><td>train/dfl_loss</td><td>▁</td></tr><tr><td>val/box_loss</td><td>▁</td></tr><tr><td>val/cls_loss</td><td>▁</td></tr><tr><td>val/dfl_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00333</td></tr><tr><td>lr/pg1</td><td>0.00333</td></tr><tr><td>lr/pg2</td><td>0.00333</td></tr><tr><td>metrics/mAP50(B)</td><td>0.44227</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.30404</td></tr><tr><td>metrics/precision(B)</td><td>0.55582</td></tr><tr><td>metrics/recall(B)</td><td>0.4211</td></tr><tr><td>model/GFLOPs</td><td>8.858</td></tr><tr><td>model/parameters</td><td>3157200</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>1.507</td></tr><tr><td>train/box_loss</td><td>1.13337</td></tr><tr><td>train/cls_loss</td><td>1.41261</td></tr><tr><td>train/dfl_loss</td><td>1.18053</td></tr><tr><td>val/box_loss</td><td>1.20597</td></tr><tr><td>val/cls_loss</td><td>1.40341</td></tr><tr><td>val/dfl_loss</td><td>1.21164</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">scarlet-sound-1</strong> at: <a href='https://wandb.ai/enxo7899/yolov8-project-pretrained/runs/6oj1rwer' target=\"_blank\">https://wandb.ai/enxo7899/yolov8-project-pretrained/runs/6oj1rwer</a><br/> View project at: <a href='https://wandb.ai/enxo7899/yolov8-project-pretrained' target=\"_blank\">https://wandb.ai/enxo7899/yolov8-project-pretrained</a><br/>Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240809_090542-6oj1rwer/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:6oj1rwer). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240809_102038-2sp33fth</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/enxo7899/yolov8-project-pretrained/runs/2sp33fth' target=\"_blank\">sage-wood-2</a></strong> to <a href='https://wandb.ai/enxo7899/yolov8-project-pretrained' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/enxo7899/yolov8-project-pretrained' target=\"_blank\">https://wandb.ai/enxo7899/yolov8-project-pretrained</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/enxo7899/yolov8-project-pretrained/runs/2sp33fth' target=\"_blank\">https://wandb.ai/enxo7899/yolov8-project-pretrained/runs/2sp33fth</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=coco.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "Model summary: 225 layers, 3,157,200 parameters, 3,157,184 gradients, 8.9 GFLOPs\n",
            "\n",
            "Transferred 355/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/coco/labels/train2017.cache... 117266 images, 1021 backgrounds, 0 corrupt: 100%|██████████| 118287/118287 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100%|██████████| 5000/5000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 8 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 100 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      1/100       4.7G      1.133      1.413      1.181        204        640: 100%|██████████| 7393/7393 [36:15<00:00,  3.40it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:27<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all       5000      36335      0.556      0.421      0.442      0.304\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "      2/100      5.71G      1.229      1.647      1.243        184        640:  62%|██████▏   | 4584/7393 [21:15<11:20,  4.13it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPqADdXfp5WL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}